{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602f063e-7064-4b7f-b9d7-d67a921a0df1",
   "metadata": {},
   "source": [
    "<img src=\"../Images/pyspark_iceberg.png\" width =\"500\" height=500> </img>\n",
    "<h3>Data Quality With Pyspark And Iceberg: WAP (write-audit-publish) methodology</h3>\n",
    "<ul>\n",
    "    <li><h4>I recommend run this notebook on google colab, there, pyspark is preinstalled and iceberg jar files are ready to go and it's easier to set it up.</h4></li>\n",
    "    <li><h4>I use spark, version 3.5.3 and iceberg, artifact version 3.5_2.12:1.7.1</h4></li>\n",
    "    <li><h4>These spark and iceberg versions are compatible with each other, if you want to use any other spark versions check this compatibility to avoid errors.</h4></li>\n",
    "    <li><h4>for downloading iceberg runtime and extension jar files check this link and download proper versions: <a href=\"https://repo1.maven.org/maven2/org/apache/iceberg/\">iceberg mavens</a></h4></li>\n",
    "    <li><h4>I use new york yellow taxis trip data for Jan 2024, the original data was too big so I made a random sample from it in data folder in current repo, but if you need full data click on this link to download it: <a href=\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"> new york city yellow taxis trip data</a></h4></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745342c-fc56-4656-8517-124f142f5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark import SparkConf, SparkContext \n",
    "\n",
    "# Initialize Spark and Iceberg configurations\n",
    "warehouse_path = \"./warehouse\"\n",
    "catalog_name = \"demo\"\n",
    "iceberg_spark_jar = 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1'\n",
    "iceberg_spark_ext = 'org.apache.iceberg:iceberg-spark-extensions-3.5_2.12:1.7.1'\n",
    "\n",
    "conf = SparkConf().setAppName(\"Data Quality - WAP termo - Apache Iceberg\") \\\n",
    "    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .set(\"spark.jars.packages\" , iceberg_spark_jar + \",\" + iceberg_spark_ext) \\\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}.warehouse\", warehouse_path) \\\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}.type\", \"hadoop\")\\\n",
    "    .set(\"spark.sql.defaultCatalog\", catalog_name)\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67b265-4648-4ff1-9457-a88378fd37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "database_name = \"nyc\"\n",
    "table_name = \"yellow_taxi_trips\"\n",
    "write_table_name = \"yellow_taxi_trips_write\"\n",
    "\n",
    "table_full_name = database_name + \".\" + table_name\n",
    "write_table_full_name = database_name + \".\" + write_table_name\n",
    "\n",
    "# nyc_yellow_trip data is for initializing our table and the other one is for write section in which our data will change and we want to audit it.\n",
    "data_dir_path = \"../Data/nyc_yellow_trip_data_2024.csv\"\n",
    "write_data_dir_path = \"../Data/nyc_yellow_trip_data_2024_only_to_write.csv\"\n",
    "\n",
    "# we will have two branches, one of them is main and users could see it, and the other is etl_job_v_1 which we want to audit\n",
    "branch_name = \"etl_job_v_1\"\n",
    "main_branch_name = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c3c56-4b2f-438e-b872-7c8da1bdd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Drop database and table if they exist (to ensure clean runs)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_full_name}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {write_table_full_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdaebb3-dd3e-4510-895c-67a153d83484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the Iceberg table\n",
    "df = spark.read.option(\"multiline\",\"true\") \\\n",
    "          .csv(data_dir_path, header=True)\n",
    "\n",
    "# save dataframe as an iceberg table\n",
    "df.write.saveAsTable(table_full_name)\n",
    "\n",
    "# check data, in write section, we will write data to our audit branch, so there should be two vendorIds in table, but after write section\n",
    "# there should be three different vendorId in table on this branch.\n",
    "spark.sql(f\"\"\"\n",
    "SELECT vendorId, sum(total_amount) vendor_total_amount\n",
    "FROM {table_full_name}\n",
    "GROUP BY VendorId\n",
    "\"\"\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059c7d1-6bc7-41ed-8653-abebf3e1d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable wap on table by using TBLPROPERTIES.\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_full_name}\n",
    "SET TBLPROPERTIES (\n",
    "    'write.wap.enabled'='true'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# create a new branch to write data and audit it.\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_full_name}\n",
    "CREATE BRANCH {branch_name}\n",
    "\"\"\")\n",
    "\n",
    "# config spark and set this new branch as its context branch name\n",
    "spark.conf.set('spark.wap.branch', branch_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01be9f-f277-40ab-86c7-fe1a41aefa0b",
   "metadata": {},
   "source": [
    "<h4>Write Section</h4>\n",
    "\n",
    "<h5>writing operations can include various actions such as deleting data, adding new data, schema evolution, any possible change on your data which you want to audit</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e35687-38f4-4cae-b48e-282eb30ff743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read to write data from csv file and make spark dataframe out of it.\n",
    "write_df = spark.read.option(\"multiline\" , \"true\")\\\n",
    "                .csv(write_data_dir_path, header= True)\n",
    "\n",
    "# save it as an iceberg table \n",
    "write_df.write.saveAsTable(write_table_full_name)\n",
    "\n",
    "# write new data to main table\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {table_full_name}\n",
    "SELECT * FROM {write_table_full_name}\n",
    "\"\"\")\n",
    "\n",
    "# check that this added data is only on new branch > we should see 3 vendorId here \n",
    "spark.sql(f\"\"\"\n",
    "SELECT VendorId, sum(total_amount) vendor_total_amount\n",
    "FROM  {table_full_name} VERSION AS OF '{branch_name}'\n",
    "GROUP BY VendorId\n",
    "\"\"\").show(5,False)\n",
    "\n",
    "# check main branch, it shouldnt has the new data, because we didnt audit and publish it to main branch yet! > we should see 2 vendorId here.\n",
    "spark.sql(f\"\"\"\n",
    "SELECT VendorId, sum(total_amount) vendor_total_amount\n",
    "FROM {table_full_name} VERSION AS OF '{main_branch_name}'\n",
    "GROUP BY VendorId\n",
    "\"\"\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4ad79-e28a-4b2b-954b-afa49528dfe5",
   "metadata": {},
   "source": [
    "<h4>Audit Section</h4>\n",
    "\n",
    "<h5>In Audit section we could check data healthiness, so you could run any queries on your data, also you could use <strong>greate expectations</strong> library to validate your expectations from added data, if you dont see any error or exception, then your tests and audits are passed and you could publish it to main branch, so your users could see new data, and if you see any error you could investigate your data and see the problem before publish section</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500408d2-e1bd-4431-8c99-94b320553f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we expect to have 3 different vendorID (1,2,6) so if we see only this three ids then audit passed else audit failed.\n",
    "# another expectation here could be len of distinct of vendorID column, so if len is not 3 then audit failed.\n",
    "to_audit_df = spark.read \\\n",
    "    .option(\"branch\", branch_name) \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(table_full_name) \\\n",
    "    .select(\"vendorID\") \\\n",
    "    .distinct() \\\n",
    "    .toLocalIterator()\n",
    "\n",
    "vendor_ids = {row[0] for row in to_audit_df}\n",
    "\n",
    "# our expectations \n",
    "expected_values = {\"1\" , \"2\" , \"6\"}\n",
    "number_of_different_vendor_ids = 3\n",
    "\n",
    "# audit\n",
    "if  (len(vendor_ids) != number_of_different_vendor_ids) \\\n",
    "    or \\\n",
    "    (len(vendor_ids) != len(set.union(expected_values, vendor_ids))):\n",
    "  raise ValueError(\"Audit Failed!\")\n",
    "else:\n",
    "  print(\"Audit Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637680b-7510-4d3a-bbe5-daeef021b3a2",
   "metadata": {},
   "source": [
    "<h4>Publish Section</h4>\n",
    "<h5>In publish we want to merge our branch with main branch, in spark, documentations recommend to use <strong>fast_forward</strong> function to do so!</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe125a35-ea74-429b-861f-f83283717268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish etl_job_v_1 branch to main branch\n",
    "publish_query = f\"CALL demo.system.fast_forward('{table_full_name}', '{main_branch_name}', '{branch_name}')\"\n",
    "spark.sql(publish_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
